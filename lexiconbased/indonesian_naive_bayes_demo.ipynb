{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indonesian Naive Bayes Sentiment Analyzer Demo\n",
    "This notebook demonstrates the usage of the Indonesian Naive Bayes Sentiment Analyzer that combines multiple Indonesian lexicon resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from indonesian_naive_bayes_analyzer import IndonesianNaiveBayesAnalyzer, create_sample_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Analyzer\n",
    "The analyzer will automatically load multiple Indonesian lexicon resources from GitHub repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Indonesian Naive Bayes Analyzer\n",
    "print(\"Initializing Indonesian Naive Bayes Sentiment Analyzer...\")\n",
    "analyzer = IndonesianNaiveBayesAnalyzer()\n",
    "\n",
    "# Display loaded lexicons\n",
    "print(f\"\\nLoaded {len(analyzer.lexicons)} lexicon resources:\")\n",
    "for name, lexicon in analyzer.lexicons.items():\n",
    "    print(f\"- {name}: {len(lexicon)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset\n",
    "We'll create a sample Indonesian sentiment dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "texts, labels = create_sample_dataset()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "df_sample = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'sentiment': labels\n",
    "})\n",
    "\n",
    "print(f\"Sample dataset created with {len(df_sample)} examples\")\n",
    "print(f\"\\nSentiment distribution:\")\n",
    "print(df_sample['sentiment'].value_counts())\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "display(df_sample.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Lexicon Coverage\n",
    "Let's analyze how well our lexicons cover the vocabulary in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze lexicon coverage\n",
    "coverage_results = analyzer.analyze_lexicon_coverage(texts)\n",
    "\n",
    "# Visualize lexicon hits\n",
    "lexicon_hits = coverage_results['lexicon_hits']\n",
    "if lexicon_hits:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(lexicon_hits.keys(), lexicon_hits.values())\n",
    "    plt.title('Lexicon Hits in Sample Dataset')\n",
    "    plt.xlabel('Lexicon Source')\n",
    "    plt.ylabel('Number of Hits')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(f\"\\nOverall lexicon coverage: {coverage_results['coverage']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Lexicon Features\n",
    "Let's examine the lexicon-based features extracted from sample texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for sample texts\n",
    "sample_texts = [\n",
    "    \"Film ini sangat bagus dan menghibur sekali\",\n",
    "    \"Saya tidak suka dengan film ini, sangat buruk\",\n",
    "    \"Film yang biasa saja, tidak terlalu istimewa\"\n",
    "]\n",
    "\n",
    "print(\"Lexicon Features Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "feature_data = []\n",
    "for i, text in enumerate(sample_texts):\n",
    "    features = analyzer.extract_lexicon_features(text)\n",
    "    feature_data.append(features)\n",
    "    \n",
    "    print(f\"\\nText {i+1}: {text}\")\n",
    "    print(f\"Sentiment Score: {features['sentiment_score']:.3f}\")\n",
    "    print(f\"Positive Words: {features['positive_count']}\")\n",
    "    print(f\"Negative Words: {features['negative_count']}\")\n",
    "    print(f\"Booster Words: {features['booster_count']}\")\n",
    "    print(f\"Negation Words: {features['negation_count']}\")\n",
    "\n",
    "# Create DataFrame for features\n",
    "df_features = pd.DataFrame(feature_data)\n",
    "df_features['text'] = sample_texts\n",
    "\n",
    "# Visualize features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Sentiment scores\n",
    "axes[0,0].bar(range(len(sample_texts)), df_features['sentiment_score'])\n",
    "axes[0,0].set_title('Sentiment Scores')\n",
    "axes[0,0].set_xlabel('Text Index')\n",
    "axes[0,0].set_ylabel('Score')\n",
    "\n",
    "# Word counts\n",
    "word_counts = df_features[['positive_count', 'negative_count', 'booster_count', 'negation_count']]\n",
    "word_counts.plot(kind='bar', ax=axes[0,1])\n",
    "axes[0,1].set_title('Word Counts by Type')\n",
    "axes[0,1].set_xlabel('Text Index')\n",
    "axes[0,1].legend()\n",
    "\n",
    "# Positive vs Negative scores\n",
    "axes[1,0].scatter(df_features['positive_score'], df_features['negative_score'])\n",
    "axes[1,0].set_title('Positive vs Negative Scores')\n",
    "axes[1,0].set_xlabel('Positive Score')\n",
    "axes[1,0].set_ylabel('Negative Score')\n",
    "\n",
    "# Feature correlation\n",
    "feature_cols = ['positive_score', 'negative_score', 'sentiment_score', 'positive_count', 'negative_count']\n",
    "corr_matrix = df_features[feature_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=axes[1,1])\n",
    "axes[1,1].set_title('Feature Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Naive Bayes Model\n",
    "Now let's train the Naive Bayes model using the combined lexicon and TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Training the Indonesian Naive Bayes model...\")\n",
    "training_results = analyzer.train(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining Results:\")\n",
    "print(f\"Accuracy: {training_results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Predictions\n",
    "Let's test the trained model on various Indonesian texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test texts\n",
    "test_texts = [\n",
    "    \"Film ini sangat bagus dan menghibur sekali, saya suka banget!\",\n",
    "    \"Saya tidak suka dengan film ini, sangat buruk dan mengecewakan\",\n",
    "    \"Film yang biasa saja, tidak terlalu istimewa tapi lumayan\",\n",
    "    \"Ceritanya hebat dan aktingnya luar biasa, sangat memuaskan\",\n",
    "    \"Sangat membosankan, saya tidak akan merekomendasikan film ini\",\n",
    "    \"Film yang cukup bagus untuk ditonton bersama keluarga\",\n",
    "    \"Tidak ada yang menarik dari film ini, membuang waktu saja\",\n",
    "    \"Saya sangat terkesan dengan kualitas produksinya yang mantap\"\n",
    "]\n",
    "\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = []\n",
    "for i, text in enumerate(test_texts):\n",
    "    result = analyzer.predict_sentiment(text)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"\\nText {i+1}: {text}\")\n",
    "    print(f\"Predicted Sentiment: {result['sentiment']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"Lexicon Score: {result['lexicon_score']:.3f}\")\n",
    "    print(f\"Positive Words: {result['positive_words']}, Negative Words: {result['negative_words']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Create results DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'text': test_texts,\n",
    "    'predicted_sentiment': [r['sentiment'] for r in results],\n",
    "    'confidence': [r['confidence'] for r in results],\n",
    "    'lexicon_score': [r['lexicon_score'] for r in results]\n",
    "})\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_counts = df_results['predicted_sentiment'].value_counts()\n",
    "axes[0].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "axes[0].set_title('Predicted Sentiment Distribution')\n",
    "\n",
    "# Confidence scores\n",
    "axes[1].bar(range(len(test_texts)), df_results['confidence'])\n",
    "axes[1].set_title('Prediction Confidence')\n",
    "axes[1].set_xlabel('Text Index')\n",
    "axes[1].set_ylabel('Confidence')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Lexicon scores vs confidence\n",
    "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'blue'}\n",
    "for sentiment in df_results['predicted_sentiment'].unique():\n",
    "    mask = df_results['predicted_sentiment'] == sentiment\n",
    "    axes[2].scatter(df_results[mask]['lexicon_score'], df_results[mask]['confidence'], \n",
    "                   c=colors.get(sentiment, 'gray'), label=sentiment, alpha=0.7)\n",
    "\n",
    "axes[2].set_title('Lexicon Score vs Confidence')\n",
    "axes[2].set_xlabel('Lexicon Score')\n",
    "axes[2].set_ylabel('Confidence')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Performance Analysis\n",
    "Let's analyze the model's performance in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed classification report\n",
    "if 'classification_report' in training_results:\n",
    "    report = training_results['classification_report']\n",
    "    \n",
    "    print(\"Detailed Classification Report:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create DataFrame for better visualization\n",
    "    metrics_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Remove support column for cleaner display\n",
    "    if 'support' in metrics_df.columns:\n",
    "        display_df = metrics_df.drop('support', axis=1)\n",
    "    else:\n",
    "        display_df = metrics_df\n",
    "    \n",
    "    display(display_df.round(3))\n",
    "    \n",
    "    # Visualize metrics\n",
    "    if len(display_df) > 3:  # Ensure we have class-specific metrics\n",
    "        class_metrics = display_df.iloc[:-3]  # Exclude macro avg, weighted avg, accuracy\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Precision\n",
    "        class_metrics['precision'].plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "        axes[0].set_title('Precision by Class')\n",
    "        axes[0].set_ylabel('Precision')\n",
    "        axes[0].set_ylim(0, 1)\n",
    "        \n",
    "        # Recall\n",
    "        class_metrics['recall'].plot(kind='bar', ax=axes[1], color='lightcoral')\n",
    "        axes[1].set_title('Recall by Class')\n",
    "        axes[1].set_ylabel('Recall')\n",
    "        axes[1].set_ylim(0, 1)\n",
    "        \n",
    "        # F1-score\n",
    "        class_metrics['f1-score'].plot(kind='bar', ax=axes[2], color='lightgreen')\n",
    "        axes[2].set_title('F1-Score by Class')\n",
    "        axes[2].set_ylabel('F1-Score')\n",
    "        axes[2].set_ylim(0, 1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"- Lexicon sources: {len(analyzer.lexicons)}\")\n",
    "print(f\"- Total lexicon entries: {sum(len(lex) for lex in analyzer.lexicons.values())}\")\n",
    "print(f\"- Training accuracy: {training_results['accuracy']:.4f}\")\n",
    "print(f\"- Feature combination: Lexicon + TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and Load Model\n",
    "Demonstrate how to save and load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'indonesian_nb_sentiment_model.pkl'\n",
    "analyzer.save_model(model_path)\n",
    "\n",
    "# Load the model (demonstration)\n",
    "new_analyzer = IndonesianNaiveBayesAnalyzer()\n",
    "new_analyzer.load_model(model_path)\n",
    "\n",
    "# Test the loaded model\n",
    "test_text = \"Film ini sangat bagus dan menghibur\"\n",
    "result = new_analyzer.predict_sentiment(test_text)\n",
    "\n",
    "print(f\"Testing loaded model:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "\n",
    "print(f\"\\nModel successfully saved and loaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
