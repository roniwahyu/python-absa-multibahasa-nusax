{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect-Based Sentiment Analysis with Multi-Class SVM\n",
    "\n",
    "This notebook implements aspect-based sentiment analysis using:\n",
    "- Feature extraction: TF-IDF and Word2Vec\n",
    "- Sentiment labeling: Score-based, TextBlob, VADER, and Ensemble methods\n",
    "- Classification: Multi-Class SVM with various kernels\n",
    "- Data splits: 65%, 70%, 75% training scenarios\n",
    "- Evaluation: Confusion Matrix and ROC/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('google_play_reviews_DigitalBank_sentiment_analysis.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration and visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Sentiment Distribution Across Different Methods', fontsize=16)\n",
    "\n",
    "sentiment_columns = ['sentiment_score_based', 'sentiment_textblob', 'sentiment_vader', 'sentiment_ensemble']\n",
    "\n",
    "for i, col in enumerate(sentiment_columns):\n",
    "    ax = axes[i//2, i%2]\n",
    "    df[col].value_counts().plot(kind='bar', ax=ax, title=col.replace('_', ' ').title())\n",
    "    ax.set_xlabel('Sentiment')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distribution statistics\n",
    "for col in sentiment_columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts())\n",
    "    print(f\"Percentage distribution:\")\n",
    "    print(df[col].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text data\n",
    "texts = df['stemmed_text'].fillna('').astype(str)\n",
    "\n",
    "# 1. TF-IDF Feature Extraction\n",
    "print(\"Extracting TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    stop_words=None  # Already preprocessed\n",
    ")\n",
    "\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(texts)\n",
    "print(f\"TF-IDF feature shape: {tfidf_features.shape}\")\n",
    "\n",
    "# 2. Word2Vec Feature Extraction\n",
    "print(\"\\nTraining Word2Vec model...\")\n",
    "# Tokenize texts for Word2Vec\n",
    "tokenized_texts = [simple_preprocess(text) for text in texts]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "print(f\"Word2Vec vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create document vectors from Word2Vec\n",
    "def get_document_vector(tokens, model, vector_size=100):\n",
    "    \"\"\"Create document vector by averaging word vectors\"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv.key_to_index:\n",
    "            vectors.append(model.wv[token])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Create Word2Vec features\n",
    "print(\"Creating Word2Vec document vectors...\")\n",
    "w2v_features = np.array([get_document_vector(tokens, w2v_model) for tokens in tokenized_texts])\n",
    "print(f\"Word2Vec feature shape: {w2v_features.shape}\")\n",
    "\n",
    "# Convert TF-IDF to dense array for consistency\n",
    "tfidf_features_dense = tfidf_features.toarray()\n",
    "print(f\"TF-IDF dense feature shape: {tfidf_features_dense.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Class SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variables\n",
    "sentiment_methods = ['sentiment_score_based', 'sentiment_textblob', 'sentiment_vader', 'sentiment_ensemble']\n",
    "feature_types = ['TF-IDF', 'Word2Vec']\n",
    "kernels = ['linear', 'rbf', 'poly', 'sigmoid']\n",
    "train_sizes = [0.65, 0.70, 0.75]\n",
    "\n",
    "# Encode labels\n",
    "label_encoders = {}\n",
    "encoded_labels = {}\n",
    "\n",
    "for method in sentiment_methods:\n",
    "    le = LabelEncoder()\n",
    "    encoded_labels[method] = le.fit_transform(df[method])\n",
    "    label_encoders[method] = le\n",
    "    print(f\"{method} classes: {le.classes_}\")\n",
    "\n",
    "print(\"\\nLabel encoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate SVM model\n",
    "def evaluate_svm_model(X_train, X_test, y_train, y_test, kernel, method_name, feature_name, train_size):\n",
    "    \"\"\"Train and evaluate SVM model\"\"\"\n",
    "    \n",
    "    # Train SVM\n",
    "    svm_model = SVC(kernel=kernel, probability=True, random_state=42)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    y_pred_proba = svm_model.predict_proba(X_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Classification report\n",
    "    class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # ROC AUC (for multiclass)\n",
    "    try:\n",
    "        if len(np.unique(y_test)) > 2:\n",
    "            # Multiclass ROC AUC\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='weighted')\n",
    "        else:\n",
    "            # Binary ROC AUC\n",
    "            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "    except:\n",
    "        roc_auc = None\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'feature': feature_name,\n",
    "        'kernel': kernel,\n",
    "        'train_size': train_size,\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'classification_report': class_report,\n",
    "        'confusion_matrix': cm,\n",
    "        'model': svm_model\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "results = []\n",
    "feature_data = {\n",
    "    'TF-IDF': tfidf_features_dense,\n",
    "    'Word2Vec': w2v_features\n",
    "}\n",
    "\n",
    "print(\"Starting comprehensive SVM evaluation...\")\n",
    "print(\"This may take several minutes...\\n\")\n",
    "\n",
    "total_experiments = len(sentiment_methods) * len(feature_types) * len(kernels) * len(train_sizes)\n",
    "current_experiment = 0\n",
    "\n",
    "for method in sentiment_methods:\n",
    "    y = encoded_labels[method]\n",
    "    \n",
    "    for feature_name, X in feature_data.items():\n",
    "        for train_size in train_sizes:\n",
    "            test_size = 1 - train_size\n",
    "            \n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, train_size=train_size, random_state=42, stratify=y\n",
    "            )\n",
    "            \n",
    "            for kernel in kernels:\n",
    "                current_experiment += 1\n",
    "                print(f\"Experiment {current_experiment}/{total_experiments}: {method} | {feature_name} | {kernel} | Train: {train_size*100:.0f}%\")\n",
    "                \n",
    "                try:\n",
    "                    result = evaluate_svm_model(\n",
    "                        X_train, X_test, y_train, y_test, \n",
    "                        kernel, method, feature_name, train_size\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in experiment: {e}\")\n",
    "                    continue\n",
    "\n",
    "print(f\"\\nCompleted {len(results)} successful experiments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': r['method'],\n",
    "        'Feature': r['feature'],\n",
    "        'Kernel': r['kernel'],\n",
    "        'Train_Size': r['train_size'],\n",
    "        'Accuracy': r['accuracy'],\n",
    "        'ROC_AUC': r['roc_auc']\n",
    "    }\n",
    "    for r in results\n",
    "])\n",
    "\n",
    "print(\"Results Summary:\")\n",
    "print(results_df.groupby(['Method', 'Feature', 'Kernel']).agg({\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'ROC_AUC': ['mean', 'std']\n",
    "}).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Accuracy comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "fig.suptitle('SVM Performance Comparison Across Different Configurations', fontsize=16)\n",
    "\n",
    "# 1. Accuracy by Method and Feature\n",
    "ax1 = axes[0, 0]\n",
    "sns.boxplot(data=results_df, x='Method', y='Accuracy', hue='Feature', ax=ax1)\n",
    "ax1.set_title('Accuracy by Sentiment Method and Feature Type')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Accuracy by Kernel\n",
    "ax2 = axes[0, 1]\n",
    "sns.boxplot(data=results_df, x='Kernel', y='Accuracy', ax=ax2)\n",
    "ax2.set_title('Accuracy by SVM Kernel')\n",
    "\n",
    "# 3. Accuracy by Training Size\n",
    "ax3 = axes[1, 0]\n",
    "sns.boxplot(data=results_df, x='Train_Size', y='Accuracy', ax=ax3)\n",
    "ax3.set_title('Accuracy by Training Size')\n",
    "\n",
    "# 4. ROC AUC by Method and Feature\n",
    "ax4 = axes[1, 1]\n",
    "results_df_clean = results_df.dropna(subset=['ROC_AUC'])\n",
    "sns.boxplot(data=results_df_clean, x='Method', y='ROC_AUC', hue='Feature', ax=ax4)\n",
    "ax4.set_title('ROC AUC by Sentiment Method and Feature Type')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best performing models\n",
    "print(\"Top 10 Best Performing Models by Accuracy:\")\n",
    "top_accuracy = results_df.nlargest(10, 'Accuracy')\n",
    "print(top_accuracy[['Method', 'Feature', 'Kernel', 'Train_Size', 'Accuracy', 'ROC_AUC']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Top 10 Best Performing Models by ROC AUC:\")\n",
    "top_roc = results_df.dropna(subset=['ROC_AUC']).nlargest(10, 'ROC_AUC')\n",
    "print(top_roc[['Method', 'Feature', 'Kernel', 'Train_Size', 'Accuracy', 'ROC_AUC']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of best model\n",
    "best_model_idx = results_df['Accuracy'].idxmax()\n",
    "best_result = results[best_model_idx]\n",
    "\n",
    "print(f\"Best Model Configuration:\")\n",
    "print(f\"Method: {best_result['method']}\")\n",
    "print(f\"Feature: {best_result['feature']}\")\n",
    "print(f\"Kernel: {best_result['kernel']}\")\n",
    "print(f\"Training Size: {best_result['train_size']*100:.0f}%\")\n",
    "print(f\"Accuracy: {best_result['accuracy']:.4f}\")\n",
    "print(f\"ROC AUC: {best_result['roc_auc']:.4f}\")\n",
    "\n",
    "# Get actual test data for detailed report\n",
    "method = best_result['method']\n",
    "feature_name = best_result['feature']\n",
    "train_size = best_result['train_size']\n",
    "\n",
    "y = encoded_labels[method]\n",
    "X = feature_data[feature_name]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=train_size, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train best model\n",
    "best_svm = SVC(kernel=best_result['kernel'], probability=True, random_state=42)\n",
    "best_svm.fit(X_train, y_train)\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "# Classification report for best model\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "class_names = label_encoders[method].classes_\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Visualization for Best Models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Confusion Matrices for Best Models by Sentiment Method', fontsize=16)\n",
    "\n",
    "# Get best model for each sentiment method\n",
    "best_models_by_method = results_df.loc[results_df.groupby('Method')['Accuracy'].idxmax()]\n",
    "\n",
    "for i, (_, row) in enumerate(best_models_by_method.iterrows()):\n",
    "    if i >= 4:  # Only show first 4\n",
    "        break\n",
    "        \n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Find corresponding result\n",
    "    result = next(r for r in results if (\n",
    "        r['method'] == row['Method'] and \n",
    "        r['feature'] == row['Feature'] and \n",
    "        r['kernel'] == row['Kernel'] and \n",
    "        r['train_size'] == row['Train_Size']\n",
    "    ))\n",
    "    \n",
    "    cm = result['confusion_matrix']\n",
    "    class_names = label_encoders[result['method']].classes_\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(f\"{row['Method']}\\n{row['Feature']} | {row['Kernel']} | Acc: {row['Accuracy']:.3f}\")\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary\n",
    "print(\"=\" * 80)\n",
    "print(\"ASPECT-BASED SENTIMENT ANALYSIS - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"- Total samples: {len(df)}\")\n",
    "print(f\"- Features extracted: TF-IDF ({tfidf_features.shape[1]} features), Word2Vec (100 dimensions)\")\n",
    "print(f\"- Sentiment methods compared: {len(sentiment_methods)}\")\n",
    "print(f\"- SVM kernels tested: {kernels}\")\n",
    "print(f\"- Training sizes tested: {[f'{size*100:.0f}%' for size in train_sizes]}\")\n",
    "print(f\"- Total experiments conducted: {len(results)}\")\n",
    "\n",
    "print(f\"\\nOverall Performance Summary:\")\n",
    "print(f\"- Best accuracy achieved: {results_df['Accuracy'].max():.4f}\")\n",
    "print(f\"- Average accuracy across all experiments: {results_df['Accuracy'].mean():.4f} ± {results_df['Accuracy'].std():.4f}\")\n",
    "if not results_df['ROC_AUC'].isna().all():\n",
    "    print(f\"- Best ROC AUC achieved: {results_df['ROC_AUC'].max():.4f}\")\n",
    "    print(f\"- Average ROC AUC: {results_df['ROC_AUC'].mean():.4f} ± {results_df['ROC_AUC'].std():.4f}\")\n",
    "\n",
    "print(f\"\\nBest Performing Configuration:\")\n",
    "best_config = results_df.loc[results_df['Accuracy'].idxmax()]\n",
    "print(f\"- Sentiment Method: {best_config['Method']}\")\n",
    "print(f\"- Feature Type: {best_config['Feature']}\")\n",
    "print(f\"- SVM Kernel: {best_config['Kernel']}\")\n",
    "print(f\"- Training Size: {best_config['Train_Size']*100:.0f}%\")\n",
    "print(f\"- Accuracy: {best_config['Accuracy']:.4f}\")\n",
    "if pd.notna(best_config['ROC_AUC']):\n",
    "    print(f\"- ROC AUC: {best_config['ROC_AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "# Feature type performance\n",
    "feature_performance = results_df.groupby('Feature')['Accuracy'].mean()\n",
    "best_feature = feature_performance.idxmax()\n",
    "print(f\"- Best feature type: {best_feature} (avg accuracy: {feature_performance[best_feature]:.4f})\")\n",
    "\n",
    "# Kernel performance\n",
    "kernel_performance = results_df.groupby('Kernel')['Accuracy'].mean()\n",
    "best_kernel = kernel_performance.idxmax()\n",
    "print(f\"- Best SVM kernel: {best_kernel} (avg accuracy: {kernel_performance[best_kernel]:.4f})\")\n",
    "\n",
    "# Training size impact\n",
    "size_performance = results_df.groupby('Train_Size')['Accuracy'].mean()\n",
    "best_size = size_performance.idxmax()\n",
    "print(f\"- Optimal training size: {best_size*100:.0f}% (avg accuracy: {size_performance[best_size]:.4f})\")\n",
    "\n",
    "# Sentiment method performance\n",
    "method_performance = results_df.groupby('Method')['Accuracy'].mean()\n",
    "best_method = method_performance.idxmax()\n",
    "print(f\"- Best sentiment method: {best_method} (avg accuracy: {method_performance[best_method]:.4f})\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"1. Use {best_feature} features for better performance\")\n",
    "print(f\"2. {best_kernel} kernel shows best results for this dataset\")\n",
    "print(f\"3. {best_size*100:.0f}% training split provides optimal balance\")\n",
    "print(f\"4. {best_method} sentiment labeling method is most suitable\")\n",
    "print(f\"5. Consider ensemble methods for production deployment\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis completed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
