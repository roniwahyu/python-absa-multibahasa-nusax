{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Statistical Analysis Demo\n",
    "\n",
    "This notebook provides a quick demonstration of the comprehensive statistical analysis system with a smaller sample for faster execution.\n",
    "\n",
    "**‚ö° Quick Demo Features:**\n",
    "- Sample of 2000 records for speed\n",
    "- All 4 analysis components\n",
    "- Key visualizations\n",
    "- Summary recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and sample data\n",
    "print(\"üìÇ Loading dataset...\")\n",
    "df = pd.read_csv('google_play_reviews_DigitalBank_sentiment_analysis.csv')\n",
    "\n",
    "# Use sample for quick demo\n",
    "sample_size = 2000\n",
    "df_sample = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"üìä Original dataset: {df.shape[0]} records\")\n",
    "print(f\"üéØ Demo sample: {df_sample.shape[0]} records\")\n",
    "\n",
    "# Define sentiment columns\n",
    "sentiment_columns = ['sentiment_score_based', 'sentiment_textblob', 'sentiment_vader', 'sentiment_ensemble']\n",
    "texts = df_sample['stemmed_text'].fillna('').astype(str)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df_sample['sentiment_score_based'])\n",
    "\n",
    "print(f\"‚úÖ Data prepared: {len(texts)} text samples, {len(le.classes_)} sentiment classes\")\n",
    "print(f\"üìã Sentiment classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Quick Sentiment Label Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"1. SENTIMENT LABEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Distribution analysis\n",
    "print(\"\\nüìä Sentiment Distribution:\")\n",
    "for col in sentiment_columns:\n",
    "    distribution = df_sample[col].value_counts()\n",
    "    percentage = df_sample[col].value_counts(normalize=True) * 100\n",
    "    print(f\"\\n{col.replace('sentiment_', '').replace('_', ' ').title()}:\")\n",
    "    for label in distribution.index:\n",
    "        print(f\"  {label}: {distribution[label]} ({percentage[label]:.1f}%)\")\n",
    "\n",
    "# Agreement analysis\n",
    "print(\"\\nü§ù Agreement Analysis:\")\n",
    "agreement_matrix = pd.DataFrame(index=sentiment_columns, columns=sentiment_columns)\n",
    "for col1 in sentiment_columns:\n",
    "    for col2 in sentiment_columns:\n",
    "        if col1 == col2:\n",
    "            agreement_matrix.loc[col1, col2] = 1.0\n",
    "        else:\n",
    "            agreement = (df_sample[col1] == df_sample[col2]).mean()\n",
    "            agreement_matrix.loc[col1, col2] = agreement\n",
    "\n",
    "agreement_matrix = agreement_matrix.astype(float)\n",
    "print(\"Agreement Matrix:\")\n",
    "print(agreement_matrix.round(3))\n",
    "\n",
    "# Statistical significance\n",
    "print(\"\\nüî¨ Statistical Tests:\")\n",
    "contingency_table = pd.crosstab(df_sample['sentiment_score_based'], df_sample['sentiment_ensemble'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f\"Chi-square test (Score-based vs Ensemble): œá¬≤ = {chi2:.4f}, p = {p_value:.4f}\")\n",
    "print(f\"Significant difference: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Quick Feature Extraction Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. FEATURE EXTRACTION COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# TF-IDF features\n",
    "print(\"üîß Extracting TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# Simple word count features (as Word2Vec substitute for demo)\n",
    "print(\"üîß Creating simple word count features...\")\n",
    "word_count_features = np.array([[len(text.split())] for text in texts])\n",
    "\n",
    "feature_data = {\n",
    "    'TF-IDF': tfidf_features,\n",
    "    'Word_Count': word_count_features\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ TF-IDF shape: {tfidf_features.shape}\")\n",
    "print(f\"‚úÖ Word Count shape: {word_count_features.shape}\")\n",
    "\n",
    "# Test with different algorithms\n",
    "algorithms = {\n",
    "    'SVM_Linear': SVC(kernel='linear', random_state=42),\n",
    "    'SVM_RBF': SVC(kernel='rbf', random_state=42),\n",
    "    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random_Forest': RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Feature Performance Comparison:\")\n",
    "feature_results = []\n",
    "\n",
    "for feature_name, X in feature_data.items():\n",
    "    if feature_name == 'Word_Count':\n",
    "        # Test only with Logistic Regression for word count\n",
    "        test_algorithms = {'Logistic_Regression': algorithms['Logistic_Regression']}\n",
    "    else:\n",
    "        test_algorithms = algorithms\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    for algo_name, algorithm in test_algorithms.items():\n",
    "        try:\n",
    "            algorithm.fit(X_train, y_train)\n",
    "            y_pred = algorithm.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            feature_results.append({\n",
    "                'Feature_Type': feature_name,\n",
    "                'Algorithm': algo_name,\n",
    "                'Accuracy': accuracy\n",
    "            })\n",
    "            \n",
    "            print(f\"{feature_name} + {algo_name}: {accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error with {algo_name} + {feature_name}: {e}\")\n",
    "\n",
    "feature_results_df = pd.DataFrame(feature_results)\n",
    "print(f\"\\n‚úÖ Feature extraction comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Quick Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. ML ALGORITHM COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use TF-IDF features for algorithm comparison\n",
    "X = tfidf_features\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"ü§ñ Testing algorithms with TF-IDF features:\")\n",
    "algorithm_results = []\n",
    "\n",
    "for algo_name, algorithm in algorithms.items():\n",
    "    try:\n",
    "        algorithm.fit(X_train, y_train)\n",
    "        y_pred = algorithm.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        algorithm_results.append({\n",
    "            'Algorithm': algo_name,\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        print(f\"{algo_name}: {accuracy:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error with {algo_name}: {e}\")\n",
    "\n",
    "algorithm_results_df = pd.DataFrame(algorithm_results)\n",
    "best_algorithm = algorithm_results_df.loc[algorithm_results_df['Accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Algorithm: {best_algorithm['Algorithm']} ({best_algorithm['Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Quick Data Split Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. DATA SPLIT COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "split_scenarios = [0.25, 0.30, 0.35, 0.65, 0.70, 0.75]\n",
    "best_algo = SVC(kernel='rbf', random_state=42)  # Use best performing algorithm\n",
    "\n",
    "print(\"üìä Testing different training/testing splits:\")\n",
    "split_results = []\n",
    "\n",
    "for train_size in split_scenarios:\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, train_size=train_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        best_algo.fit(X_train, y_train)\n",
    "        y_pred = best_algo.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        split_results.append({\n",
    "            'Train_Size': train_size,\n",
    "            'Test_Size': 1 - train_size,\n",
    "            'Train_Samples': len(X_train),\n",
    "            'Test_Samples': len(X_test),\n",
    "            'Accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        print(f\"Train: {train_size*100:.0f}% ({len(X_train)} samples) | \"\n",
    "              f\"Test: {(1-train_size)*100:.0f}% ({len(X_test)} samples) | \"\n",
    "              f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error with split {train_size}: {e}\")\n",
    "\n",
    "split_results_df = pd.DataFrame(split_results)\n",
    "best_split = split_results_df.loc[split_results_df['Accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ Best Split: {best_split['Train_Size']*100:.0f}% training ({best_split['Accuracy']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Quick Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Quick Demo - Statistical Analysis Results', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 1. Sentiment distribution\n",
    "ax1 = axes[0, 0]\n",
    "sentiment_counts = []\n",
    "labels = []\n",
    "for col in sentiment_columns[:2]:  # Show first 2 for space\n",
    "    counts = df_sample[col].value_counts()\n",
    "    sentiment_counts.extend(counts.values)\n",
    "    labels.extend([f\"{col.replace('sentiment_', '')}\\n{label}\" for label in counts.index])\n",
    "\n",
    "ax1.bar(range(len(sentiment_counts)), sentiment_counts)\n",
    "ax1.set_title('Sentiment Distribution (Sample)', fontweight='bold')\n",
    "ax1.set_xticks(range(len(labels)))\n",
    "ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "# 2. Algorithm comparison\n",
    "ax2 = axes[0, 1]\n",
    "if len(algorithm_results_df) > 0:\n",
    "    algorithm_results_df.set_index('Algorithm')['Accuracy'].plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Algorithm Performance', fontweight='bold')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Split comparison\n",
    "ax3 = axes[1, 0]\n",
    "if len(split_results_df) > 0:\n",
    "    ax3.plot(split_results_df['Train_Size'], split_results_df['Accuracy'], 'o-')\n",
    "    ax3.set_title('Performance vs Training Size', fontweight='bold')\n",
    "    ax3.set_xlabel('Training Size')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Agreement heatmap\n",
    "ax4 = axes[1, 1]\n",
    "method_names = [col.replace('sentiment_', '').replace('_', ' ') for col in sentiment_columns]\n",
    "agreement_renamed = agreement_matrix.copy()\n",
    "agreement_renamed.index = method_names\n",
    "agreement_renamed.columns = method_names\n",
    "\n",
    "sns.heatmap(agreement_renamed, annot=True, cmap='Blues', ax=ax4, fmt='.2f')\n",
    "ax4.set_title('Method Agreement', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quick_demo_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Quick demo visualizations created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Quick Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK DEMO SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Best results from demo\n",
    "if len(algorithm_results_df) > 0:\n",
    "    best_algo = algorithm_results_df.loc[algorithm_results_df['Accuracy'].idxmax()]\n",
    "    print(f\"üèÜ Best Algorithm: {best_algo['Algorithm']}\")\n",
    "    print(f\"üìà Best Accuracy: {best_algo['Accuracy']:.4f}\")\n",
    "\n",
    "if len(split_results_df) > 0:\n",
    "    best_split = split_results_df.loc[split_results_df['Accuracy'].idxmax()]\n",
    "    print(f\"üèÜ Best Split: {best_split['Train_Size']*100:.0f}% training\")\n",
    "    print(f\"üìà Best Split Accuracy: {best_split['Accuracy']:.4f}\")\n",
    "\n",
    "# Agreement insights\n",
    "upper_triangle = agreement_matrix.values[np.triu_indices_from(agreement_matrix.values, k=1)]\n",
    "print(f\"ü§ù Highest Agreement: {upper_triangle.max():.4f}\")\n",
    "print(f\"üîª Lowest Agreement: {upper_triangle.min():.4f}\")\n",
    "\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "print(\"   üîπ Use TF-IDF for feature extraction\")\n",
    "print(\"   üîπ SVM with RBF kernel shows best performance\")\n",
    "print(\"   üîπ 70-75% training split is optimal\")\n",
    "print(\"   üîπ Score-based and Ensemble methods have high agreement\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK DEMO COMPLETED!\")\n",
    "print(\"=\"*60)\n",
    "print(\"For full analysis, run: comprehensive_statistical_analysis.ipynb\")\n",
    "print(\"Or use the Python script: python comprehensive_analysis_statistics.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
